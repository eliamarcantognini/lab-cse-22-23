{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7243a-b7d3-4e47-b3ec-49cafdebada5",
   "metadata": {},
   "source": [
    "# 301 Spark basics\n",
    "\n",
    "The goal of this lab is to get familiar with Spark programming.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "\n",
    "## 301-2 Running a sample Spark job\n",
    "\n",
    "Goal: calculate the average temperature for every month; dataset is ```weather-sample1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323446c-bdfe-4eff-b87a-ce57f7c840b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val bucketname = \"unibo-bd2223-egallinucci\"\n",
    "\n",
    "val rddWeather = sc.textFile(\"s3a://\"+bucketname+\"/datasets/weather-sample1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5830c-1cd8-4f3a-bfc5-f91e9aa8d87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parseWeatherLine(line:String):(String,Double) = {\n",
    "  val year = line.substring(15,19)\n",
    "  val month = line.substring(19,21)\n",
    "  val day = line.substring(21,23)\n",
    "  var temp = line.substring(87,92).toInt\n",
    "  (month, temp/10)\n",
    "}\n",
    "\n",
    "// Parse records\n",
    "val rddWeatherKv = rddWeather.map(x => parseWeatherLine(x))\n",
    "// Aggregate by key (i.e., month) to compute the sum and the count of temperature values\n",
    "val rddTempDataPerMonth = rddWeatherKv.aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2))\n",
    "// Calculate the average temperature in each record\n",
    "val rddAvgTempPerMonth = rddTempDataPerMonth.map({case(k,v) => (k, v._1/v._2)})\n",
    "// Sort, coalesce and cache the result (because it is used twice)\n",
    "val rddCached = rddAvgTempPerMonth.sortByKey().coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621e29d-4c97-4779-b7d1-e5126bce5d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Show all the records\n",
    "rddCached.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d2419-c6a0-4616-be1c-c24f09653107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rddCached.saveAsTextFile(\"s3a://\"+bucketname+\"/spark/301-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b8fdd-ef59-4e64-b749-1d9ae50260bc",
   "metadata": {},
   "source": [
    "## 301-3 Spark warm-up\n",
    "\n",
    "Load the ```capra``` and ```divinacommedia``` datasets and try the following actions:\n",
    "- Show their content (```collect```)\n",
    "- Count their rows (```count```)\n",
    "- Split phrases into words (```map``` or ```flatMap```; what’s the difference?)\n",
    "- Check the results (remember: evaluation is lazy)\n",
    "- Try the ```toDebugString``` function to check the execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52649088-9a57-437b-9127-0d29984b4e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rddCapra = sc.textFile(\"s3a://\"+bucketname+\"/datasets/capra.txt\")\n",
    "val rddDC = sc.textFile(\"s3a://\"+bucketname+\"/datasets/divinacommedia.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d9363-3ce0-411b-a826-ee3cceb935a1",
   "metadata": {},
   "source": [
    "## 301-4 From MapReduce to Spark\n",
    "\n",
    "Reproduce on Spark the exercises seen on Hadoop MapReduce on the capra and divinacommedia datasets.\n",
    "\n",
    "- Jobs:\n",
    "  - Count the number of occurrences of each word\n",
    "    - Result: (sopra, 1), (la, 4), …\n",
    "  - Count the number of occurrences of words of given lengths\n",
    "    - Result: (2, 4), (5, 8)\n",
    "  - Count the average length of words given their first letter (hint: check the example in 301-1)\n",
    "    - Result: (s, 5), (l, 2), …\n",
    "  - Return the inverted index of words\n",
    "    - Result: (sopra, (0)), (la, (0, 1)), ...\n",
    "- How does Spark compare with respect to MapReduce? (performance, ease of use)\n",
    "- How is the output sorted? How can you sort by value?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
